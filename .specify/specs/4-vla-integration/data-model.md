# Data Model: Vision-Language-Action (VLA) Integration

## Overview
This document defines the key data entities and their relationships for the Vision-Language-Action (VLA) integration system. These entities represent the core concepts that will be manipulated by the VLA pipeline.

## Entity: VoiceCommand
**Description**: Natural language input from human user processed by the VLA system

### Attributes
- `id`: string (required) - Unique identifier for the command
- `audio_data`: bytes (optional) - Raw audio input (may be processed and not stored)
- `transcribed_text`: string (required) - Text representation from speech recognition
- `confidence`: float (0.0-1.0) (required) - Confidence score of transcription
- `timestamp`: datetime (required) - When the command was received
- `user_id`: string (optional) - Identifier of the user issuing the command
- `language_code`: string (optional) - Language of the original command (default: "en")
- `processed_status`: enum (required) - Current processing state: ["received", "processing", "completed", "failed"]
- `error_message`: string (optional) - Error details if processing failed

### Validation Rules
- `confidence` must be between 0.0 and 1.0
- `transcribed_text` must not be empty when status is "completed"
- `timestamp` must be in the past or present
- `processed_status` must be one of the defined enum values

### Relationships
- One-to-many with `TaskPlan` (one command can generate multiple plans in different contexts)
- One-to-many with `ActionExecution` (through the associated TaskPlan)

## Entity: TaskPlan
**Description**: Sequence of actions generated by LLM from natural language command

### Attributes
- `id`: string (required) - Unique identifier for the plan
- `source_command_id`: string (required) - Reference to the original voice command
- `action_sequence`: array of ActionDefinition (required) - Ordered list of ROS 2 actions to execute
- `status`: enum (required) - Current execution status: ["planned", "executing", "completed", "failed", "cancelled"]
- `created_timestamp`: datetime (required) - When the plan was generated
- `start_timestamp`: datetime (optional) - When execution started
- `completion_timestamp`: datetime (optional) - When execution completed
- `robot_capabilities`: string (optional) - Capabilities considered during planning
- `execution_feedback`: string (optional) - Runtime feedback during execution
- `priority`: integer (optional) - Execution priority level (0-10, default: 5)

### Validation Rules
- `action_sequence` must contain at least one action
- `status` must be one of the defined enum values
- `created_timestamp` must be before `start_timestamp` and `completion_timestamp`
- `priority` must be between 0 and 10

### ActionDefinition Structure
```
{
  "action_type": string (required) - Type of action (navigation, manipulation, perception, etc.)
  "parameters": object (required) - Specific parameters for the action
  "timeout": float (optional) - Maximum time for action execution in seconds
  "fallback_actions": array of ActionDefinition (optional) - Actions to try if this fails
  "dependencies": array of string (optional) - IDs of actions that must complete first
}
```

### Relationships
- Many-to-one with `VoiceCommand` (many plans can come from one command in different contexts)
- One-to-many with `ActionExecution` (one plan can contain multiple action executions)

## Entity: PerceptionData
**Description**: Visual and sensor information from the robot's environment

### Attributes
- `id`: string (required) - Unique identifier for the perception data
- `sensor_type`: enum (required) - Type of sensor data: ["camera", "lidar", "imu", "depth", "audio", "other"]
- `data_content`: object (required) - The actual sensor data (structure varies by sensor type)
- `timestamp`: datetime (required) - When the data was captured
- `robot_pose`: object (optional) - Robot's pose when data was captured {x, y, z, qx, qy, qz, qw}
- `environment_context`: string (optional) - Contextual information about the environment
- `processed_objects`: array of object (optional) - Recognized objects and their properties
- `quality_score`: float (0.0-1.0) (optional) - Quality assessment of the data
- `frame_id`: string (optional) - Coordinate frame identifier

### Validation Rules
- `sensor_type` must be one of the defined enum values
- `quality_score` must be between 0.0 and 1.0 if provided
- `timestamp` must be in the past or present

### Processed Object Structure
```
{
  "object_id": string (required) - Unique identifier for the object
  "object_type": string (required) - Type of object (e.g., "cup", "chair", "person")
  "position": {x: float, y: float, z: float} (required) - 3D position in space
  "confidence": float (0.0-1.0) (required) - Recognition confidence
  "properties": object (optional) - Additional properties (color, size, etc.)
}
```

### Relationships
- One-to-many with `ActionExecution` (perception data used as input for actions)
- Many-to-one with `TaskPlan` (perception data influences planning decisions)

## Entity: ActionExecution
**Description**: Specific ROS 2 command sent to robot hardware

### Attributes
- `id`: string (required) - Unique identifier for the action
- `action_definition`: ActionDefinition (required) - The action to execute
- `target_robot_id`: string (required) - Identifier of the robot executing the action
- `execution_status`: enum (required) - Current status: ["pending", "executing", "completed", "failed", "cancelled"]
- `start_time`: datetime (optional) - When execution started
- `completion_time`: datetime (optional) - When execution completed
- `execution_result`: object (optional) - Result of the action execution
- `error_message`: string (optional) - Error details if execution failed
- `feedback_data`: array of object (optional) - Runtime feedback during execution
- `associated_perception_data`: array of string (optional) - IDs of perception data used

### Validation Rules
- `execution_status` must be one of the defined enum values
- `start_time` must be before `completion_time` if both are present
- `action_definition` must have a valid action_type

### Feedback Data Structure
```
{
  "timestamp": datetime (required) - When feedback was recorded
  "feedback_type": string (required) - Type of feedback (e.g., "progress", "obstacle", "error")
  "data": object (required) - Feedback content specific to the type
}
```

### Relationships
- Many-to-one with `TaskPlan` (many actions can be part of one plan)
- Many-to-one with `PerceptionData` (actions may use perception data as input)
- Many-to-one with `VoiceCommand` (through the associated TaskPlan)

## Entity Relationships Diagram

```
VoiceCommand (1) ←→ (M) TaskPlan ←→ (M) ActionExecution
                    ↓
               (M) PerceptionData ←→ (M) ActionExecution
```

## State Transitions

### TaskPlan States
- planned → executing: When plan execution begins
- executing → completed: When all actions in the plan complete successfully
- executing → failed: When one or more actions fail and no fallbacks are available
- executing → cancelled: When execution is manually cancelled
- planned → cancelled: When plan is cancelled before execution starts

### ActionExecution States
- pending → executing: When action execution begins
- executing → completed: When action completes successfully
- executing → failed: When action execution fails
- pending → cancelled: When action is cancelled before execution
- executing → cancelled: When action is cancelled during execution

## Data Flow Patterns

### Voice Command Processing Flow
VoiceCommand (received) → TaskPlan (planned) → ActionExecution (executing/completed)

### Perception Integration Flow
PerceptionData (captured) → TaskPlan (influences planning) → ActionExecution (uses perception data)

### Execution Feedback Flow
ActionExecution (feedback) → TaskPlan (updates status) → VoiceCommand (completion status)